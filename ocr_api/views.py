from django.shortcuts import render
from rest_framework.response import Response
from rest_framework.decorators import api_view
from rest_framework import permissions
from rest_framework.parsers import MultiPartParser, FormParser

from main_api.service import MainApiService
from ocr_api.service import OCRApiService

from .serializers import ImgSerializer
from .models import ImgData

from keras.models import load_model
from keras.preprocessing import image

from django.conf import settings

# Line Segmentation

import cv2 as cv
from urllib.request import urlopen
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import keras
from keras.layers import Conv2D, Activation, MaxPooling2D, Dense, Dropout, Flatten
from keras.models import model_from_json
import os
from aksharamukha import transliterate

# from models.get_preds import get_predictions

# Create your views here.
@api_view(['GET'])
def apiOverview(request):
  api_urls = {
      'Send Image to Model': '/ocr_api/send_image',
      'Access Image from database': '/ocr_api/get_image?<id>',
  }
  return Response(api_urls)


@api_view(['POST'])
def save_image(request):
    data = request.data
    ocr_service = OCRApiService()
    file = request.FILES
    res = ocr_service.send_image_to_model(data, file)
    return Response(res)


@api_view(['GET'])
def get_image(request):
    ocr_service = OCRApiService()
    img_id = request.query_params['id']
    img_obj = ocr_service.get_image_details(img_id)
    image_path = "./media/" + img_obj['image']
    img_obj['img_pred'] = get_preds(image_path)
    print("This is prediction --->> ", img_obj['img_pred'])
    print("This is result --->> ", img_obj)
    return Response(img_obj)


def prepare(file):
    IMG_SIZE = 224
    image = cv.imread(file)
    image_resized = cv.resize(image, (224, 224))
    image = np.expand_dims(image_resized, axis=0)
    return image


def findPeakRegions(vpp,divider):
    peaks = []
    threshold = (np.max(vpp)-np.min(vpp))/divider
    # print(threshold)
    peaks = []
    peaks_index = []
    for i, vppv in enumerate(vpp):
        # print(vppv)
        if vppv > threshold:
            peaks.append([i, vppv])
    return peaks


def get_preds(input_image):
    img_path = input_image

    img = cv.imread(img_path)

    for filename in os.listdir('./models/Segmentation/characters'):
        os.remove(f'./models/Segmentation/characters/{filename}')
    for filename in os.listdir('./models/Segmentation/lines'):
        os.remove(f'./models/Segmentation/lines/{filename}')

    # for i in range(len(lIndexB)):
    #     os.remove(f"{line_dir}line{i+1}.jpg")
    # for i in range(num_chars):
    #     os.remove(f"{char_dir}char{i+1}.png")

    # binarisation
    blur = cv.medianBlur(img, 7)
    grayImg = cv.cvtColor(blur, cv.COLOR_BGR2GRAY)
    ret3, th3 = cv.threshold(grayImg, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)
    img = cv.bitwise_not(th3)
    cv.imwrite("./models/result.jpeg", img)

    print("this is img " , img)

    hProj = np.sum(img,1)

    divider = 0.01
    while 1:
        try:
            peaks = findPeakRegions(hProj,divider)
            peaksIndex = np.array(peaks)[:, 0].astype(int)
            break
        except:
            divider+=0.01

        # print(divider)
    

    segmentedImg = np.copy(img)
    r,c = segmentedImg.shape

    for ri in range(r):
        if ri in peaksIndex:
            segmentedImg[ri, :] = 0     

    hProjLines = np.sum(segmentedImg,1)
    hProjLines = np.append(hProjLines,[0,0,0])

    lines = []
    lIndexB = []
    lIndexE = []

    for ri in range(len(hProjLines)):
        if hProjLines[ri]!=0 and hProjLines[ri-1]==0 and hProjLines[ri-2]==0:
            lIndexB.append(ri)
        if hProjLines[ri]!=0 and hProjLines[ri+1]==0 and hProjLines[ri+2]==0:
            lIndexE.append(ri)
            
    for i in range(len(lIndexB)):
        lines.append(img[lIndexB[i]:lIndexE[i],:])
        cv.imwrite("./models/Segmentation/lines/line{}.jpg".format(i+1),lines[i])

    class_names = ['a',
    'ba',
    'be',
    'bha',
    'bhe',
    'bhi',
    'bho',
    'bhu',
    'bhÄ',
    'bhÄ«',
    'bhÅ«',
    'bi',
    'bo',
    'bu',
    'bÄ',
    'ca',
    'ce',
    'cha',
    'che',
    'chi',
    'chu',
    'chÄ',
    'ci',
    'co',
    'cu',
    'cÄ',
    'cÅ«',
    'da',
    'de',
    'dha',
    'dhe',
    'dhi',
    'dho',
    'dhu',
    'dhÄ',
    'dhÄ«',
    'dhÅ«',
    'di',
    'do',
    'du',
    'dÄ',
    'dÄ«',
    'e',
    'ga',
    'ge',
    'gha',
    'ghe',
    'gho',
    'ghu',
    'ghÄ',
    'gi',
    'go',
    'gu',
    'gÄ',
    'ha',
    'he',
    'hi',
    'ho',
    'hu',
    'hÄ',
    'hÄ«',
    'hÅ«',
    'i',
    'ja',
    'je',
    'jha',
    'jhi',
    'jhÄ',
    'ji',
    'jo',
    'ju',
    'jÄ',
    'jÄ«',
    'jÅ«',
    'ka',
    'ke',
    'kha',
    'khe',
    'khi',
    'kho',
    'khu',
    'khÄ',
    'khÄ«',
    'ki',
    'ko',
    'ku',
    'kÄ',
    'kÄ«',
    'kÅ«',
    'la',
    'le',
    'li',
    'lo',
    'lu',
    'lÄ',
    'lÄ«',
    'lÅ«',
    'ma',
    'me',
    'mi',
    'mo',
    'mu',
    'mÄ',
    'mÄ«',
    'mÅ«',
    'na',
    'ne',
    'ni',
    'no',
    'nu',
    'nÄ',
    'nÄ«',
    'nÅ«',
    'o',
    'pa',
    'pe',
    'pha',
    'phe',
    'phÄ',
    'pi',
    'po',
    'pu',
    'pÄ',
    'pÄ«',
    'ra',
    're',
    'ri',
    'ro',
    'ru',
    'rÄ',
    'rÄ«',
    'rÅ«',
    'sa',
    'se',
    'si',
    'so',
    'su',
    'sÄ',
    'sÄ«',
    'sÅ«',
    'ta',
    'te',
    'tha',
    'the',
    'thi',
    'thu',
    'thÄ',
    'thÄ«',
    'ti',
    'to',
    'tu',
    'tÄ',
    'tÄ«',
    'tÅ«',
    'u',
    'va',
    've',
    'vi',
    'vo',
    'vu',
    'vÄ',
    'vÄ«',
    'vÅ«',
    'ya',
    'ye',
    'yi',
    'yo',
    'yu',
    'yÄ',
    'yÄ«',
    'yÅ«',
    'Ã±a',
    'Ã±e',
    'Ã±o',
    'Ã±Ä',
    'Ä',
    'Å›a',
    'Å›i',
    'Å›u',
    'Å›Ä',
    'á¸a',
    'á¸e',
    'á¸ha',
    'á¸he',
    'á¸hi',
    'á¸hÄ',
    'á¸hÄ«',
    'á¸i',
    'á¸u',
    'á¸Ä',
    'á¸Ä«',
    'á¹‡a',
    'á¹‡e',
    'á¹‡i',
    'á¹‡Ä',
    'á¹‡Ä«',
    'á¹£a',
    'á¹£e',
    'á¹£i',
    'á¹£o',
    'á¹£u',
    'á¹£Ä',
    'á¹­a','á¹­e','á¹­ha','á¹­he','á¹­hi','á¹­hÄ','á¹­hÄ«','á¹­hÅ«','á¹­i','á¹­u','á¹­Ä','á¹­Ä«']

    brahmi_dict = {'a':'ð‘€…','Ä':'ð‘€†','i':'ð‘€‡','Ä«':'ð‘€ˆ','u':'ð‘€‰','Å«':'ð‘€Š','e':'ð‘€','ai':'ð‘€','o':'ð‘€‘','au':'ð‘€’','aá¹ƒ':'ð‘€…ð‘€',
    'ka':'ð‘€“','kÄ':'ð‘€“ð‘€¸','ki':'ð‘€“ð‘€º','kÄ«':'ð‘€“ð‘€»','ku':'ð‘€“ð‘€¼','kÅ«':'ð‘€“ð‘€½','ke':'ð‘€“ð‘‚','kai':'ð‘€“ð‘ƒ','ko':'ð‘€“ð‘„','kau':'ð‘€“ð‘…','kaá¹ƒ':'ð‘€“ð‘€',
    'kha':'ð‘€”â€‹','khÄ':'ð‘€”ð‘€¸','khi':'ð‘€”ð‘€º','khÄ«':'ð‘€”ð‘€»','khu':'ð‘€”ð‘€¼','khÅ«':'ð‘€”ð‘€½','khe':'ð‘€”ð‘‚','khai':'ð‘€”ð‘ƒ','kho':'ð‘€”ð‘„','khau':'ð‘€”ð‘…','khaá¹ƒ':'ð‘€”ð‘€',
    'ga':'ð‘€•â€‹','gÄ':'ð‘€•ð‘€¸','gi':'ð‘€•ð‘€º','gÄ«':'ð‘€•ð‘€»','gu':'ð‘€•ð‘€¼','gÅ«':'ð‘€•ð‘€½','ge':'ð‘€•ð‘‚','gai':'ð‘€•ð‘ƒ','go':'ð‘€•ð‘„','gau':'ð‘€•ð‘…','gaá¹ƒ':'ð‘€•ð‘€',
    'gha':'ð‘€–â€‹','ghÄ':'ð‘€–ð‘€¸','ghi':'ð‘€–ð‘€º','ghÄ«':'ð‘€–ð‘€»','ghu':'ð‘€–ð‘€¼','ghÅ«':'ð‘€–ð‘€½','ghe':'ð‘€–ð‘‚','ghai':'ð‘€–ð‘ƒ','gho':'ð‘€–ð‘„','ghau':'ð‘€–ð‘…','ghaá¹ƒ':'ð‘€–ð‘€',
    'á¹…a':'ð‘€—â€‹','á¹…Ä':'ð‘€—ð‘€¸','á¹…i':'ð‘€—ð‘€º','á¹…Ä«':'ð‘€—ð‘€»','á¹…u':'ð‘€—ð‘€¼','á¹…Å«':'ð‘€—ð‘€½','á¹…e':'ð‘€—ð‘‚','á¹…ai':'ð‘€—ð‘ƒ','á¹…o':'ð‘€—ð‘„','á¹…au':'ð‘€—ð‘…','á¹…aá¹ƒ':'ð‘€—ð‘€',
    'ca':'ð‘€˜â€‹','cÄ':'ð‘€˜ð‘€¸','ci':'ð‘€˜ð‘€º','cÄ«':'ð‘€˜ð‘€»','cu':'ð‘€˜ð‘€¼','cÅ«':'ð‘€˜ð‘€½','ce':'ð‘€˜ð‘‚','cai':'ð‘€˜ð‘ƒ','co':'ð‘€˜ð‘„','cau':'ð‘€˜ð‘…','caá¹ƒ':'ð‘€˜ð‘€',
    'cha':'ð‘€™â€‹','chÄ':'ð‘€™ð‘€¸','chi':'ð‘€™ð‘€º','chÄ«':'ð‘€™ð‘€»','chu':'ð‘€™ð‘€¼','chÅ«':'ð‘€™ð‘€½','che':'ð‘€™ð‘‚','chai':'ð‘€™ð‘ƒ','cho':'ð‘€™ð‘„','chau':'ð‘€™ð‘…','chaá¹ƒ':'ð‘€™ð‘€',
    'ja':'ð‘€š','jÄ':'ð‘€šð‘€¸','ji':'ð‘€šð‘€º','jÄ«':'ð‘€šð‘€»','ju':'ð‘€šð‘€¼','jÅ«':'ð‘€šð‘€½','je':'ð‘€šð‘‚','jai':'ð‘€šð‘ƒ','jo':'ð‘€šð‘„','jau':'ð‘€šð‘…','jaá¹ƒ':'ð‘€šð‘€',
    'jha':'ð‘€›â€‹','jhÄ':'ð‘€›ð‘€¸','jhi':'ð‘€›ð‘€º','jhÄ«':'ð‘€›ð‘€»','jhu':'ð‘€›ð‘€¼','jhÅ«':'ð‘€›ð‘€½','jhe':'ð‘€›ð‘‚','jhai':'ð‘€›ð‘ƒ','jho':'ð‘€›ð‘„','jhau':'ð‘€›ð‘…','jhaá¹ƒ':'ð‘€›ð‘€¸ð‘€',
    'Ã±a':'ð‘€œâ€‹','Ã±Ä':'ð‘€œð‘€¸','Ã±i':'ð‘€œð‘€º','Ã±Ä«':'ð‘€œð‘€»','Ã±u':'ð‘€œð‘€¼','Ã±Å«':'ð‘€œð‘€½','Ã±e':'ð‘€œð‘‚','Ã±ai':'ð‘€œð‘ƒ','Ã±o':'ð‘€œð‘„','Ã±au':'ð‘€œð‘…','Ã±aá¹ƒ':'ð‘€œð‘€',
    'á¹­a':'ð‘€â€‹','á¹­Ä':'ð‘€ð‘€¸','á¹­i':'ð‘€ð‘€º','á¹­Ä«':'ð‘€ð‘€»','á¹­u':'ð‘€ð‘€¼','á¹­Å«':'ð‘€ð‘€½','á¹­e':'ð‘€ð‘‚','á¹­ai':'ð‘€ð‘ƒ','á¹­o':'ð‘€ð‘„','á¹­au':'ð‘€ð‘…','á¹­aá¹ƒ':'ð‘€ð‘€',
    'á¹­ha':'ð‘€žâ€‹','á¹­hÄ':'ð‘€žð‘€¸','á¹­hi':'ð‘€žð‘€º','á¹­hÄ«':'ð‘€žð‘€»','á¹­hu':'ð‘€žð‘€¼','á¹­hÅ«':'ð‘€žð‘€½','á¹­he':'ð‘€žð‘‚','á¹­hai':'ð‘€žð‘ƒ','á¹­ho':'ð‘€žð‘„','á¹­hau':'ð‘€žð‘…','á¹­haá¹ƒ':'ð‘€žð‘€',
    'á¸a':'ð‘€Ÿâ€‹','á¸Ä':'ð‘€¤ð‘€¸','á¸i':'ð‘€Ÿð‘€º','á¸Ä«':'ð‘€Ÿð‘€»','á¸u':'ð‘€Ÿð‘€¼','á¸Å«':'ð‘€Ÿð‘€½','á¸e':'ð‘€Ÿð‘‚','á¸ai':'ð‘€Ÿð‘ƒ','á¸o':'ð‘€Ÿð‘„','á¸au':'ð‘€Ÿð‘…','á¸aá¹ƒ':'ð‘€Ÿð‘€',
    'á¸ha':'ð‘€ â€‹','á¸hÄ':'ð‘€ ð‘€¸','á¸hi':'ð‘€ ð‘€º','á¸hÄ«':'ð‘€ ð‘€»','á¸hu':'ð‘€ ð‘€¼','á¸hÅ«':'ð‘€ ð‘€½','á¸he':'ð‘€ ð‘‚','á¸hai':'ð‘€ ð‘ƒ','á¸ho':'ð‘€ ð‘„','á¸hau':'ð‘€ ð‘…','á¸haá¹ƒ':'ð‘€ ð‘€',
    'á¹‡a':'ð‘€¡â€‹','á¹‡Ä':'ð‘€¡ð‘€¸','á¹‡i':'ð‘€¡ð‘€º','á¹‡Ä«':'ð‘€¡ð‘€»','á¹‡u':'ð‘€¡ð‘€¼','á¹‡Å«':'ð‘€¡ð‘€½','á¹‡e':'ð‘€¡ð‘‚','á¹‡ai':'ð‘€¡ð‘ƒ','á¹‡o':'ð‘€¡ð‘„','á¹‡au':'ð‘€¡ð‘…','á¹‡aá¹ƒ':'ð‘€¡ð‘€',
    'ta':'ð‘€¢â€‹','tÄ':'ð‘€¢ð‘€¸','ti':'ð‘€¢ð‘€º','tÄ«':'ð‘€¢ð‘€»','tu':'ð‘€¢ð‘€¼','tÅ«':'ð‘€¢ð‘€½','te':'ð‘€¢ð‘‚','tai':'ð‘€¢ð‘ƒ','to':'ð‘€¢ð‘„','tau':'ð‘€¢ð‘…','taá¹ƒ':'ð‘€¢ð‘€',
    'tha':'ð‘€£â€‹','thÄ':'ð‘€£ð‘€¸','thi':'ð‘€£ð‘€º','thÄ«':'ð‘€£ð‘€»','thu':'ð‘€£ð‘€¼','thÅ«':'ð‘€£ð‘€½','the':'ð‘€£ð‘‚','thai':'ð‘€£ð‘ƒ','tho':'ð‘€£ð‘„','thau':'ð‘€£ð‘…','thaá¹ƒ':'ð‘€£ð‘€',
    'da':'ð‘€¤â€‹','dÄ':'ð‘€¤ð‘€¸','di':'ð‘€¤ð‘€º','dÄ«':'ð‘€¤ð‘€»','du':'ð‘€¤ð‘€¼','dÅ«':'ð‘€¤ð‘€½','de':'ð‘€¤ð‘‚','dai':'ð‘€¤ð‘ƒ','do':'ð‘€¤ð‘„','dau':'ð‘€¤ð‘…','daá¹ƒ':'ð‘€¤ð‘€',
    'dha':'ð‘€¥â€‹','dhÄ':'ð‘€¥ð‘€¸','dhi':'ð‘€¥ð‘€º','dhÄ«':'ð‘€¥ð‘€»','dhu':'ð‘€¥ð‘€¼','dhÅ«':'ð‘€¥ð‘€½','dhe':'ð‘€¥ð‘‚','dhai':'ð‘€¥ð‘ƒ','dho':'ð‘€¥ð‘„','dhau':'ð‘€¥ð‘…','dhaá¹ƒ':'ð‘€¥ð‘€',
    'na':'ð‘€¦â€‹','nÄ':'ð‘€¦ð‘€¸','ni':'ð‘€¦ð‘€º','nÄ«':'ð‘€¦ð‘€»','nu':'ð‘€¦ð‘€¼','nÅ«':'ð‘€¦ð‘€½','ne':'ð‘€¦ð‘‚','nai':'ð‘€¦ð‘ƒ','no':'ð‘€¦ð‘„','nau':'ð‘€¦ð‘…','naá¹ƒ':'ð‘€¦ð‘€',
    'pa':'ð‘€§â€‹','pÄ':'ð‘€§ð‘€¸','pi':'ð‘€§ð‘€º','pÄ«':'ð‘€§ð‘€»','pu':'ð‘€§ð‘€¼','pÅ«':'ð‘€§ð‘€½','pe':'ð‘€§ð‘‚','pai':'ð‘€§ð‘ƒ','po':'ð‘€§ð‘„','pau':'ð‘€§ð‘…','paá¹ƒ':'ð‘€§ð‘€',
    'pha':'ð‘€¨â€‹','phÄ':'ð‘€¨ð‘€¸','phi':'ð‘€¨ð‘€º','phÄ«':'ð‘€¨ð‘€»','phu':'ð‘€¨ð‘€¼','phÅ«':'ð‘€¨ð‘€½','phe':'ð‘€¨ð‘‚','phai':'ð‘€¨ð‘ƒ','pho':'ð‘€¨ð‘„','phau':'ð‘€¨ð‘…','phaá¹ƒ':'ð‘€¨ð‘€',
    'ba':'ð‘€©â€‹','bÄ':'ð‘€©ð‘€¸','bi':'ð‘€©ð‘€º','bÄ«':'ð‘€©ð‘€»','bu':'ð‘€©ð‘€¼','bÅ«':'ð‘€©ð‘€½','be':'ð‘€©ð‘‚','bai':'ð‘€©ð‘ƒ','bo':'ð‘€©ð‘„','bau':'ð‘€©ð‘…','baá¹ƒ':'ð‘€©ð‘€',
    'bha':'ð‘€ªâ€‹','bhÄ':'ð‘€ªð‘€¸','bhi':'ð‘€ªð‘€º','bhÄ«':'ð‘€ªð‘€»','bhu':'ð‘€ªð‘€¼','bhÅ«':'ð‘€ªð‘€½','bhe':'ð‘€ªð‘‚','bhai':'ð‘€ªð‘ƒ','bho':'ð‘€ªð‘„','bhau':'ð‘€ªð‘…','bhaá¹ƒ':'ð‘€ªð‘€',
    'ma':'ð‘€«â€‹','mÄ':'ð‘€«ð‘€¸','mi':'ð‘€«ð‘€º','mÄ«':'ð‘€«ð‘€»','mu':'ð‘€«ð‘€¼','mÅ«':'ð‘€«ð‘€½','me':'ð‘€«ð‘‚','mai':'ð‘€«ð‘ƒ','mo':'ð‘€«ð‘„','mau':'ð‘€«ð‘…','maá¹ƒ':'ð‘€«ð‘€',
    'ya':'ð‘€¬â€‹','yÄ':'ð‘€¬ð‘€¸','yi':'ð‘€¬ð‘€º','yÄ«':'ð‘€¬ð‘€»','yu':'ð‘€¬ð‘€¼','yÅ«':'ð‘€¬ð‘€½','ye':'ð‘€¬ð‘‚','yai':'ð‘€¬ð‘ƒ','yo':'ð‘€¬ð‘„','yau':'ð‘€¬ð‘…','yaá¹ƒ':'ð‘€¬ð‘€',
    'ra':'ð‘€­â€‹','rÄ':'ð‘€­â€‹','ri':'ð‘€­ð‘€º','rÄ«':'ð‘€­ð‘€»','ru':'ð‘€­ð‘€¼','rÅ«':'ð‘€­ð‘€½','re':'ð‘€­ð‘‚','rai':'ð‘€­ð‘ƒ','ro':'ð‘€­ð‘„','rau':'ð‘€­ð‘…','raá¹ƒ':'ð‘€­ð‘€',
    'la':'ð‘€®â€‹','lÄ':'ð‘€®ð‘€¸','li':'ð‘€®ð‘€º','lÄ«':'ð‘€®ð‘€»','lu':'ð‘€®ð‘€¼','lÅ«':'ð‘€®ð‘€½','le':'ð‘€®ð‘‚','lai':'ð‘€®ð‘ƒ','lo':'ð‘€®ð‘„','lau':'ð‘€®ð‘…','laá¹ƒ':'ð‘€®ð‘€',
    'va':'ð‘€¯â€‹','vÄ':'ð‘€¯ð‘€¸','vi':'ð‘€¯ð‘€º','vÄ«':'ð‘€¯ð‘€»','vu':'ð‘€¯ð‘€¼','vÅ«':'ð‘€¯ð‘€½','ve':'ð‘€¯ð‘‚','vai':'ð‘€¯ð‘ƒ','vo':'ð‘€¯ð‘„','vau':'ð‘€¯ð‘…','vaá¹ƒ':'ð‘€¯ð‘€',
    'Å›a':'ð‘€°','Å›Ä':'ð‘€°ð‘€¸','Å›i':'ð‘€°ð‘€º','Å›Ä«':'ð‘€°ð‘€»','Å›u':'ð‘€°ð‘€¼','Å›Å«':'ð‘€°ð‘€½','Å›e':'ð‘€°ð‘‚','Å›ai':'ð‘€°ð‘ƒ','Å›o':'ð‘€°ð‘„','Å›au':'ð‘€°ð‘…','Å›aá¹ƒ':'ð‘€°ð‘€',
    'á¹£a':'ð‘€±â€‹','á¹£Ä':'ð‘€±ð‘€¸','á¹£i':'ð‘€±ð‘€º','á¹£Ä«':'ð‘€±ð‘€»','á¹£u':'ð‘€±ð‘€¼','á¹£Å«':'ð‘€±ð‘€½','á¹£e':'ð‘€±ð‘‚','á¹£ai':'ð‘€±ð‘ƒ','á¹£o':'ð‘€±ð‘„','á¹£au':'ð‘€±ð‘…','á¹£aá¹ƒ':'ð‘€±ð‘€',
    'sa':'ð‘€²â€‹','sÄ':'ð‘€²ð‘€¸','si':'ð‘€²ð‘€º','sÄ«':'ð‘€²ð‘€»','su':'ð‘€²ð‘€¼','sÅ«':'ð‘€²ð‘€½','se':'ð‘€²ð‘‚','sai':'ð‘€²ð‘ƒ','so':'ð‘€²ð‘„','sau':'ð‘€²ð‘…','saá¹ƒ':'ð‘€²ð‘€',
    'ha':'ð‘€³â€‹','hÄ':'ð‘€³ð‘€¸','hi':'ð‘€³ð‘€º','hÄ«':'ð‘€³ð‘€»','hu':'ð‘€³ð‘€¼','hÅ«':'ð‘€³ð‘€½','he':'ð‘€³ð‘‚','hai':'ð‘€³ð‘ƒ','ho':'ð‘€³ð‘„','hau':'ð‘€³ð‘…','haá¹ƒ':'ð‘€³ð‘€'}

    # char_model = keras.models.load_model('./models/mobilenet_model_weights.h5')


    line_dir = './models/Segmentation/lines/'

    final_pred = ""

    kth_img = 1

    line_breaks = []

    for filename in os.listdir(line_dir):
        img = cv.imread(line_dir + filename, cv.IMREAD_GRAYSCALE)

        vProj = np.sum(img,0)

        divider = 0.01
        while 1:
            try:
                peaks = findPeakRegions(vProj,divider)
                peaksIndex = np.array(peaks)[:,0].astype(int)
                break
            except:
                divider += 0.01

        segmentedImg = np.copy(img)
        r,c = segmentedImg.shape

        for ci in range(c):
            if ci in peaksIndex:
                segmentedImg[:,ci] = 0     

        vProjLines = np.sum(segmentedImg,0)
        vProjLines = np.append(vProjLines,[0,0,0])
        chars = []
        charsB = []
        charsE = []

        for ci in range(len(vProjLines)):
            if vProjLines[ci]!=0 and vProjLines[ci-1]==0:
                charsB.append(ci)
            if vProjLines[ci]!=0 and vProjLines[ci+1]==0:
                charsE.append(ci)

        for i in range(len(charsB)):
            chars.append(img[:,charsB[i]:charsE[i]]) 
            if len(chars[i][0]) != 0 : 
                
                if(charsE[i] - charsB[i] >= 25): 
                    cv.imwrite(f"./models/Segmentation/characters/char{kth_img}.png",chars[i])  # Characters of width greater than 25 is only kept
                    kth_img += 1
    #             print(f"Segmentation/characters/char{kth_img}.png")
        line_breaks.append(kth_img-1)
        # print(line_breaks)


    char_dir = './models/Segmentation/characters/'

    num_chars = len([filename for filename in os.listdir(char_dir)])

    final_pred = ""

    model = model_from_json(open('./models/model_arch.json').read())
    model.load_weights('./models/mobilenet_model_weights.h5')

    for i in range(num_chars):
        if i in line_breaks:
            final_pred += '\n '
        img_path = f"{char_dir}char{i+1}.png"
        # print(img_path)
        img = prepare(img_path)
        pred = model.predict([img])

        pred = list(pred[0])
        char_type = class_names[pred.index(max(pred))]
        final_pred += brahmi_dict[char_type]
        final_pred += " "

    print("This is the finalll predd\n", final_pred)
    trans_text = transliterate.process('autodetect', 'Devanagari', final_pred)

    # for i in range(len(lIndexB)):
    #     os.remove(f"{line_dir}line{i+1}.jpg")
    # for i in range(num_chars):
    #     os.remove(f"{char_dir}char{i+1}.png")
        
    return trans_text












